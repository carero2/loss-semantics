{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from huggingface_hub import InferenceClient\n",
    "from termcolor import colored\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import textwrap\n",
    "import threading\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_sample = pd.read_csv('../1_data/sample1000_papers.csv')\n",
    "df_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the instances of selected\n",
    "df_sample[\"selected\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_llama_parallel(words, client, y, responses, progress_bar, system, instruct, indexes_analized, indexes_errors):\n",
    "    \n",
    "    # Template for the prompt to be sent to the language model.\n",
    "    template = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|><|start_header_id|>user<|end_header_id|>{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    \n",
    "    response = []\n",
    "    consecutives_errors = 0\n",
    "\n",
    "    indexes_viewed = []\n",
    "    indexes_errors_core = []\n",
    "\n",
    "    for i, (index, word) in enumerate(words):\n",
    "        indexes_viewed.append(index)\n",
    "\n",
    "        # Check for too many consecutive errors and interrupt if necessary (sometimes LLM model starts to hallucinate and providing non-sense responses).\n",
    "        if consecutives_errors > 50:\n",
    "            print(\"Too many consecutive errors. Interrupting...\")\n",
    "\n",
    "            responses[y] = [y, response]\n",
    "            progress_bar.close()\n",
    "            break\n",
    "\n",
    "        # Format the instruction with the current word.\n",
    "        instruct_w = instruct.format(word=word)\n",
    "        prompt = template.format(system=system, user=instruct_w)\n",
    "\n",
    "        out = None\n",
    "        error_sum = 0\n",
    "\n",
    "        # Retry the text generation until a valid response is received or the maximum number of retries is reached.\n",
    "        while out is None:\n",
    "            try:\n",
    "                # Generate text using the client.\n",
    "                out = client.text_generation(prompt, max_new_tokens=500, temperature=0.001, do_sample=False, top_p=0.01, top_k=1)\n",
    "\n",
    "                match = re.findall(r'answer\\**\\s*\\n*[=|:]*\\s*\\**(included|excluded)\\**', out.lower())\n",
    "                \n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle rate limit and other exceptions.\n",
    "                if \"Rate limit reached.\" in str(e) or \"Max retries exceeded with url\" in str(e):\n",
    "                    print(str(e))\n",
    "                    print(\"Sleeping 10 min at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    time.sleep(600)\n",
    "                else:\n",
    "                    print(\"\\n\\nSleeping 5 seconds\\n\\n\")\n",
    "                    print(str(e))\n",
    "                    time.sleep(5)\n",
    "                out = None\n",
    "\n",
    "            # If the response is not valid, retry.\n",
    "            if out is not None and not match:\n",
    "                error_sum += 1\n",
    "                \n",
    "                # If too many errors have occurred, skip the current word.\n",
    "                if error_sum > 5:\n",
    "                    print(\"Too many errors, skipping\")\n",
    "                    out = \"ERROR: \" + out\n",
    "                    print(out)\n",
    "                    break\n",
    "\n",
    "                out = None\n",
    "\n",
    "\n",
    "        # Update the consecutive error count.\n",
    "        if \"ERROR\" in out:\n",
    "            consecutives_errors += 1\n",
    "        else:\n",
    "            consecutives_errors = 0\n",
    "\n",
    "        response.append(out)\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Store the indexes of analyzed and erroneous words.\n",
    "    indexes_errors[y] = indexes_errors_core\n",
    "    indexes_analized[y] = indexes_viewed\n",
    "    responses[y] = [y, response]\n",
    "    progress_bar.close()\n",
    "\n",
    "system = \"\"\"You are a helpful assistant with the task of deciding whether or not to select scientific articles in the context of losses in decision-making \\n\\n\"\"\" \n",
    "\n",
    "instruct = \"\"\"Based on the following definitions, criteria, rules, and reasoning instructions, decide whether to include or exclude the following article.\n",
    "\n",
    "\n",
    "*Definitions*\n",
    "- Losses: A negative outcome or detrimental event perceived or experienced by an individual. Whether an outcome or event is negative or detrimental and represents a loss ultimately depends on the authors' exposition. \n",
    "- Decision-Making: The process by which an individual evaluates options and makes choices among them. This includes but is not limited to: \n",
    "    a) Deciding between options\n",
    "    b) Evaluating or judging options\n",
    "    c) Assessing risks and benefits\n",
    "    d) Weighing options\n",
    "\n",
    "    \n",
    "*Criteria*\n",
    "- Loss Focus: The article explicitly discusses losses and their role in decision-making as an important theme.\n",
    "- Individual Level: The article discusses losses related to individual/s' perceptions and experiences.\n",
    "- Irrelevant Contexts: The article mentions \"loss\" in contexts unrelated to individual decision-making, such as technical discussions on loss functions in statistical models. \n",
    "- Secondary Treatment of Losses: The article discusses losses but they are tangential or unimportant for the article's subject matter.\n",
    "- Secondary Treatment of Decision-Making: The article mentions decision-making but does NOT explicitly analyze it.\n",
    "\n",
    "*Rules*\n",
    "\n",
    "- Inclusion rule: Include the article if the definition of Loss Focus and Individual Level are met.\n",
    "- Exclusion rule: Exclude the article if 1) it does not meet Loss Focus and Individual Level criteria, OR 2) if it meets ANY of the following criteria: Irrelevant Contexts, Secondary Treatment of Losses, or Secondary Treatment of Decision-Making.\n",
    "\n",
    "*Reasoning steps*\n",
    "\n",
    "Step 1. Evaluate Definitions: Assess whether the article meets the definitions.\n",
    "Step 2. Apply Criteria: Analyze inclusion and exclusion criteria in the context of Step 1.\n",
    "Step 3. Determine Outcome: Decide whether to include or exclude the article based on Step 2.\n",
    "Step 4. Response Format: Return your response in the *exact* following format: Answer=*included* or *excluded*\n",
    "\n",
    "Scientific article:\n",
    "{word}\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n_parts):\n",
    "    n = len(lst)\n",
    "    k = n // n_parts  \n",
    "    r = n % n_parts \n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    for i in range(n_parts):\n",
    "        end = start + k + (1 if i < r else 0)\n",
    "        parts.append(lst[start:end])\n",
    "        start = end\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "workers = 5\n",
    "n_responses = workers\n",
    "llama_resp = [[] for _ in range(workers)]\n",
    "indexes_analized = [[] for _ in range(workers)]\n",
    "indexes_errors = [[] for _ in range(workers)]\n",
    "candidates_total = [(i, f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\") for i, row in df_sample.iterrows()]\n",
    "candidates = split_list(candidates_total, workers)\n",
    "\n",
    "# Create as many clients as workers and as many workers as tokens you have and differents runs oyu want\n",
    "client1 = InferenceClient(token=TOKEN1,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client2 = InferenceClient(token=TOKEN2,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client3 = InferenceClient(token=TOKEN3,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client4 = InferenceClient(token=TOKEN4,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client5 = InferenceClient(token=TOKEN5,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "\n",
    "progress_bars = [tqdm(total=len(candidates[j]), desc=f\"Progress {j}\", leave=True) for j in range(len(candidates))]\n",
    "\n",
    "\n",
    "for y in range(workers):\n",
    "    thread = threading.Thread(target=correct_llama_parallel, args=(candidates[y], eval(f'client{y+1}'), y, llama_resp, progress_bars[y], system, instruct, indexes_analized, indexes_errors))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "\n",
    "llama_resp_no_core = []\n",
    "for resp in llama_resp:\n",
    "    llama_resp_no_core.append(resp[1])\n",
    "\n",
    "llama_resp_no_core = list(itertools.chain.from_iterable(llama_resp_no_core))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean the answer and find the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_resp_clean = []\n",
    "errors = []\n",
    "\n",
    "for i, resp in enumerate(llama_resp_no_core):\n",
    "    if \"LAST ERROR\" in resp:\n",
    "        llama_resp_clean.append(\"error\")\n",
    "        errors.append(i)\n",
    "    else:\n",
    "        match = re.findall(r'answer\\**\\s*\\n*[=|:]*\\s*\\**(included|excluded)\\**', resp.lower())\n",
    "        if match:\n",
    "            llama_resp_clean.append(match[0].lower())\n",
    "\n",
    "    \n",
    "print(Counter(llama_resp_clean))\n",
    "print(errors)\n",
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_errors = split_list([(index, f\"Title: {row['Title']}\\n Abstract: {row['Abstract']}\") for index, row in df_sample.iterrows() if index in errors], workers)\n",
    "corrected_errors = [[] for _ in range(workers)]\n",
    "indexes_analized = [[] for _ in range(workers)]\n",
    "indexes_errors = [[] for _ in range(workers)]\n",
    "\n",
    "progress_bars = [tqdm(total=len(candidates_errors[j]), desc=f\"Progress {j}\", leave=True) for j in range(len(candidates_errors))]\n",
    "\n",
    "\n",
    "for y in range(workers):\n",
    "    thread = threading.Thread(target=correct_llama_parallel, args=(candidates_errors[y], eval(f'client{y+1}'), y, corrected_errors, progress_bars[y], system, instruct, indexes_analized, indexes_errors))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Flatten the list of errors\n",
    "corrected_errors_no_core = []\n",
    "for error in corrected_errors:\n",
    "    corrected_errors_no_core.append(error[1])\n",
    "\n",
    "corrected_errors_no_core = list(itertools.chain.from_iterable(corrected_errors_no_core))\n",
    "\n",
    "for i, error in enumerate(errors):\n",
    "    llama_resp_no_core[error] = corrected_errors_no_core[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[\"selected_llm\"] = llama_resp_clean\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv(\"../1_data/sample1000_papers.csv\", index=False)\n",
    "pd.DataFrame(llama_resp).to_csv(\"../1_data/reasons_normalization_papers.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes 'yes' as 1 and 'no' as 0\n",
    "le = LabelEncoder()\n",
    "human_labels = le.fit_transform(df_sample.loc[(df_sample[\"human_labeled\"] == 1), \"selected\"])\n",
    "llm_labels = le.transform(df_sample.loc[(df_sample[\"human_labeled\"] == 1), \"selected_llm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics\n",
    "precision = precision_score(human_labels, llm_labels)\n",
    "recall = recall_score(human_labels, llm_labels)\n",
    "f1 = f1_score(human_labels, llm_labels, average='weighted')\n",
    "accuracy = accuracy_score(human_labels, llm_labels)\n",
    "conf_matrix = confusion_matrix(human_labels, llm_labels)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Human: no\", \"Human: yes\"], columns=[\"LLM: no\", \"LLM: yes\"])\n",
    "\n",
    "\n",
    "# Output the results\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for i in range(len(df_sample)):\n",
    "    if df_sample.loc[i, \"selected\"] != df_sample.loc[i, \"selected_llm\"] and df_sample.loc[i, \"human_labeled\"] == 1:\n",
    "        print(f\"Error {error + 1}:\")\n",
    "        print(f\"Index: {i}\")\n",
    "        print(f\"Title: {df_sample.loc[i, \"Title\"]}\")\n",
    "        print(f\"Abstract: {textwrap.fill(df_sample.loc[i, \"Abstract\"], width=150)}\")\n",
    "        print()\n",
    "        print(\"Human: \", df_sample.loc[i, \"selected\"], \"LLM: \", df_sample.loc[i, \"selected_llm\"])\n",
    "        print(f\"Reason LLM: {textwrap.fill(llama_resp[i], width=150)}\\n\")\n",
    "        print(\"-----------------------------\")\n",
    "        error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'yes' for included and 'no' for excluded in selected_llm\n",
    "df_sample.loc[df_sample[\"human_labeled\"] == 0, \"selected_llm\"] = df_sample.loc[df_sample[\"human_labeled\"] == 0, \"selected_llm\"].replace({\"included\": \"yes\", \"excluded\": \"no\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the instances of selected\n",
    "df_sample[\"selected_llm\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_sample\n",
    "df_sample.to_csv(\"../1_data/sample1000_papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_semantics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
