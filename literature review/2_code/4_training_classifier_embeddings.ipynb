{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, train_test_split, cross_validate\n",
    "import textwrap\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('../1_data/sample1000_papers_embeddings.csv')\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column concatenating the title and abstract\n",
    "df_sample['text'] = \"Title: \" + df_sample['Title'] + '\\nAbstract: ' + df_sample['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/WhereIsAI/UAE-Large-V1\"\n",
    "headers = {\"Authorization\": \"Bearer hf_\"}\n",
    "\n",
    "def get_embedding(text):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json={\"inputs\": text})\n",
    "            response.raise_for_status()\n",
    "            output = response.json()\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            if response.status_code == 503:\n",
    "                print(f\"Request error: {e}\\nRetrying in 20 seconds...\")\n",
    "                time.sleep(5)\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Request error: {e}\\nSleeping 1hour...\")\n",
    "                time.sleep(60*60)\n",
    "        except ValueError as e:\n",
    "            print(f\"JSON decode error: {e}. Retrying in 5 seconds...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and save\n",
    "df_sample['embeddings'] = df_sample[\"text\"].apply(lambda x: get_embedding(x))\n",
    "df_sample.to_csv('../1_data/sample1000_papers_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the whole dataset\n",
    "df = pd.read_csv('../1_data/all_papers_2024-10-02.csv')\n",
    "\n",
    "# Drop the columns that are not needed\n",
    "df = df.drop(['Source title', 'Author Keywords'], axis=1)\n",
    "# Create the text\n",
    "df['text'] = \"Title: \" + df['Title'] + '\\nAbstract: ' + df['Abstract']\n",
    "print(df.shape)\n",
    "    \n",
    "# Keep only the papers that are not in the sample\n",
    "df_nosample = df[~df.ID.isin(df_sample.ID.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the whole dataset in batches and save them\n",
    "batch_size = 300\n",
    "embeddings = []\n",
    "starting_batch = 118\n",
    "for i in range(starting_batch*batch_size, len(df_nosample), batch_size):\n",
    "    print(f\"Processing batch {i//batch_size} from {i} to {i+batch_size}\")\n",
    "    batch_embeddings = df_nosample[\"text\"].iloc[i:i+batch_size].apply(lambda x: get_embedding(x)).tolist()\n",
    "    df_nosample.iloc[i:i+batch_size, df_nosample.columns.get_loc('embeddings')] = pd.Series(batch_embeddings, index=df_nosample.index[i:i+batch_size])\n",
    "    df_nosample.to_csv('../1_data/all_papers_2024-10-02_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the already created dataset\n",
    "If you already obtained embeddings and only need to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nosample = pd.read_csv('../1_data/all_papers_2024-10-02_embeddings.csv')\n",
    "df_nosample['embeddings'] = df_nosample['embeddings'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else x)\n",
    "    \n",
    "# Print how many embeddings are not nan\n",
    "print(df_nosample['embeddings'].apply(lambda x: not np.isnan(x).all()).sum())\n",
    "df_nosample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('../1_data/sample1000_papers.csv')\n",
    "print(df_sample.shape)\n",
    "df_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('../1_data/sample1000_papers.csv')\n",
    "print(df_sample.shape)\n",
    "df_sample2 = pd.read_csv('../1_data/sample1000_papers_embeddings.csv', usecols=['ID', 'embeddings'])\n",
    "df_sample2['embeddings'] = df_sample2['embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "df_sample = pd.merge(df_sample, df_sample2, on='ID', how='left')\n",
    "df_sample = pd.merge(df_sample, df_nosample[['ID', 'embeddings']], on='ID', how='left')\n",
    "df_sample['embeddings'] = df_sample['embeddings_x'].fillna(df_sample['embeddings_y'])\n",
    "\n",
    "df_sample.drop(columns=['embeddings_x', 'embeddings_y'], inplace=True)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows that have same ID as the sample\n",
    "df_nosample = df_nosample[~df_nosample.ID.isin(df_sample.ID.values)]\n",
    "df_sample2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION\n",
    "# Nested cross-validation training\n",
    "X = np.array(df_sample['embeddings'].tolist())\n",
    "y = LabelEncoder().fit_transform(df_sample['selected_llm'])\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up the Ridge Classifier and hyperparameter grid\n",
    "ridge_clf = RidgeClassifier()\n",
    "param_grid = {'alpha': np.logspace(-2, 2, 50)}\n",
    "\n",
    "# Nested cross-validation for hyperparameter tuning and model evaluation\n",
    "grid_search = GridSearchCV(estimator=ridge_clf, param_grid=param_grid, cv=inner_cv, scoring='accuracy')\n",
    "nested_scores = cross_val_score(grid_search, X, y, cv=outer_cv, scoring='accuracy')\n",
    "\n",
    "print(\"Nested CV Mean Accuracy:\", nested_scores.mean())\n",
    "print(\"Nested CV Accuracy Std Dev:\", nested_scores.std())\n",
    "\n",
    "# Example of finding the best alpha from nested CV\n",
    "best_alphas = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n",
    "    y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n",
    "    \n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    best_alphas.append(grid_search.best_params_['alpha'])\n",
    "\n",
    "# Choose the most common best alpha (or average if necessary)\n",
    "final_alpha = np.median(best_alphas)  # or use any preferred selection method\n",
    "\n",
    "print(\"Best alpha found from nested CV:\", final_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RidgeClassifier(alpha=final_alpha)\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision_weighted',\n",
    "    'recall': 'recall_weighted',\n",
    "    'f1': 'f1_weighted'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(final_model, X, y, cv=outer_cv, scoring=scoring)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"Precision: \", cv_results['test_precision'].mean())\n",
    "print(\"Recall: \", cv_results['test_recall'].mean())\n",
    "print(\"F1 Score: \", cv_results['test_f1'].mean())\n",
    "print(\"Accuracy: \", cv_results['test_accuracy'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Predict on all papers for those that have embeddings\n",
    "X_all = np.array(df_nosample[df_nosample['embeddings'].notna()]['embeddings'].tolist())\n",
    "y_all = final_model.predict(X_all)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df_nosample.loc[df_nosample['embeddings'].notna(), 'prediction'] = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count predictions\n",
    "df_nosample['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random sample of Abstracts from selected papers\n",
    "sample_abstracts = df_nosample.loc[df_nosample['prediction'] == 1, \"Abstract\"].sample(5)\n",
    "\n",
    "for i, abstract in enumerate(sample_abstracts):\n",
    "    print(f\"Abstract {i+1}:\\n\")\n",
    "    print(textwrap.fill(abstract, width=150))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected papers\n",
    "all_df = pd.concat([df_sample, df_nosample], ignore_index=True)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([all_df.loc[(all_df['human_labeled'] == 1) & (all_df['selected'] == 'yes')],\n",
    "          all_df.loc[(all_df['human_labeled'] == 0) & (all_df['selected_llm'] == 'yes')],\n",
    "          all_df.loc[all_df['prediction'] == 1]])\n",
    "print(len(total))\n",
    "total.to_csv('../1_data/selected_papers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_semantics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
