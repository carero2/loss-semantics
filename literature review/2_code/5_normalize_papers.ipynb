{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from huggingface_hub import InferenceClient\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "import threading\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = pd.read_csv('../1_data/selected_papers_normalized.csv')\n",
    "# df_selected['embeddings'] = df_selected['embeddings'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_selected.shape)\n",
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "def correct_llama_parallel(words, client, y, responses, progress_bar, system, instruct, indexes_analized, indexes_errors):\n",
    "    \"\"\"\n",
    "    Processes a list of words in parallel using a text generation client and updates the responses and progress bar.\n",
    "\n",
    "    Args:\n",
    "        words (list of tuples): A list of tuples where each tuple contains an index and a word to be processed.\n",
    "        client (object): The text generation client used to generate responses.\n",
    "        y (int): The index of the current batch being processed.\n",
    "        responses (dict): A dictionary to store the responses for each batch.\n",
    "        progress_bar (object): A progress bar object to update the progress of the processing.\n",
    "        system (str): The system prompt to be used in the text generation.\n",
    "        instruct (str): The instruction template to be used in the text generation.\n",
    "        indexes_analized (dict): A dictionary to store the indexes of the words that have been analyzed.\n",
    "        indexes_errors (dict): A dictionary to store the indexes of the words that encountered errors during processing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Template for the prompt to be sent to the language model.\n",
    "    template = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|><|start_header_id|>user<|end_header_id|>{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    \n",
    "    response = []\n",
    "    consecutives_errors = 0\n",
    "\n",
    "    indexes_viewed = []\n",
    "    indexes_errors_core = []\n",
    "\n",
    "    for i, (index, word) in enumerate(words):\n",
    "        indexes_viewed.append(index)\n",
    "\n",
    "        # Check for too many consecutive errors and interrupt if necessary (sometimes LLM model starts to hallucinate and providing non-sense responses).\n",
    "        if consecutives_errors > 10:\n",
    "            print(\"Too many consecutive errors. Interrupting...\")\n",
    "\n",
    "            responses[y] = [y, response]\n",
    "            progress_bar.close()\n",
    "            break\n",
    "\n",
    "        # Format the instruction with the current word.\n",
    "        instruct_w = instruct.format(word=word)\n",
    "        prompt = template.format(system=system, user=instruct_w)\n",
    "\n",
    "        out = None\n",
    "        error_sum = 0\n",
    "\n",
    "        # Retry the text generation until a valid response is received or the maximum number of retries is reached.\n",
    "        while out is None:\n",
    "            try:\n",
    "                # Generate text using the client.\n",
    "                out = client.text_generation(prompt, max_new_tokens=500, temperature=0.001, do_sample=False, top_p=0.01, top_k=1)\n",
    "\n",
    "                # Extract and clean the types of losses from the generated text using regular expressions.\n",
    "                match = re.search(r'(type[s]? of ?(losses|loss)\\**\\s*\\n*[=|:]*\\s*\\**)(\\S.+)', out.lower())\n",
    "                if match:\n",
    "                    text_no_parentheses = re.sub(r'\\([^)]*\\)', '', match.group(3))\n",
    "                    text_clean = re.sub(r'[^\\w\\s,]', '', text_no_parentheses)\n",
    "                    types_losses = [loss for loss in re.split(r\",\\s*\", text_clean) if loss]\n",
    "                    str_types_losses = \"none\" if \"none\" in str(types_losses).lower() else str(types_losses).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "                    out += \"\\n\" + str_types_losses\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle rate limit and other exceptions.\n",
    "                if \"Rate limit reached.\" in str(e) or \"Max retries exceeded with url\" in str(e):\n",
    "                    print(str(e))\n",
    "                    print(\"Sleeping 10 min at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    time.sleep(600)\n",
    "                else:\n",
    "                    print(\"\\n\\nSleeping 5 seconds\\n\\n\")\n",
    "                    print(str(e))\n",
    "                    time.sleep(5)\n",
    "                out = None\n",
    "\n",
    "            # If too many errors have occurred, skip the current word.\n",
    "            if out is not None and error_sum > 5:\n",
    "                print(\"Too many errors, skipping\")\n",
    "                out = \"LAST ERROR: \" + out\n",
    "                print(out)\n",
    "                indexes_errors_core.append(index)\n",
    "                break\n",
    "\n",
    "            # Check if the generated text is valid (i.e. if it was able to extract the loss type and if the number of loss types extracted is less than 3).\n",
    "            if out is not None and (not match or len(types_losses) > 3):\n",
    "                error_sum += 1\n",
    "                out = None\n",
    "\n",
    "        # Update the consecutive error count.\n",
    "        if \"LAST ERROR\" in out:\n",
    "            consecutives_errors += 1\n",
    "        else:\n",
    "            consecutives_errors = 0\n",
    "\n",
    "        response.append(out)\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Store the indexes of analyzed and erroneous words.\n",
    "    indexes_errors[y] = indexes_errors_core\n",
    "    indexes_analized[y] = indexes_viewed\n",
    "    responses[y] = [y, response]\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage (replace with your actual values)\n",
    "system = \"You are a helpful assistant specialized in summarizing scientific articles that studied a type of loss in decision-making.\"\n",
    "\n",
    "instruct = \"\"\"For the following scientific article, provide both (1) a concise one-sentence summary of the article's main content and (2) the specific type of loss that is explicitly studied or used in the article.\n",
    "\n",
    "For example, if the article uses monetary incentives to study choices between gambles, your response would be:\n",
    "*Summary*: The article studies the decision-making in monetary gambles.\n",
    "*Type of loss*: Financial\n",
    "\n",
    "Use concise, simple and everyday language.\n",
    "\n",
    "If multiple types of losses are used, list ONLY the most important ones for the study (and ONLY up to *THREE*, the rest will be discarded) separed by commas. Within the types, avoid using the word \"loss\" or similar ones. Avoid examples or clarifications between parenthesis. *Do not infer or assume any additional types beyond what is explicitly stated*.\n",
    "\n",
    "Use the *exact* following format:\n",
    "*Summary*: [summary]\n",
    "*Type of loss*: [type of loss]\n",
    "\n",
    "\n",
    "Scientific article:\n",
    "{word}\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n_parts):\n",
    "    n = len(lst)\n",
    "    k = n // n_parts  \n",
    "    r = n % n_parts \n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    for i in range(n_parts):\n",
    "        end = start + k + (1 if i < r else 0)\n",
    "        parts.append(lst[start:end])\n",
    "        start = end\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "workers = 5\n",
    "n_responses = workers\n",
    "llama_resp = [[] for _ in range(workers)]\n",
    "indexes_analized = [[] for _ in range(workers)]\n",
    "indexes_errors = [[] for _ in range(workers)]\n",
    "candidates_total = [(index, f\"Title: {row['Title']}\\n Abstract: {row['Abstract']}\") for index, row in df_selected.iterrows()]\n",
    "candidates = split_list(candidates_total, workers)\n",
    "\n",
    "# Create as many clients as workers and as many workers as tokens you have and differents runs oyu want\n",
    "client1 = InferenceClient(token=TOKEN1,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client2 = InferenceClient(token=TOKEN2,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client3 = InferenceClient(token=TOKEN3,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client4 = InferenceClient(token=TOKEN4,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "client5 = InferenceClient(token=TOKEN5,\n",
    "                         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                         headers={\"X-use-cache\": \"false\"})\n",
    "\n",
    "progress_bars = [tqdm(total=len(candidates[j]), desc=f\"Progress {j}\", leave=True) for j in range(len(candidates))]\n",
    "\n",
    "\n",
    "for y in range(workers):\n",
    "    thread = threading.Thread(target=correct_llama_parallel, args=(candidates[y], eval(f'client{y+1}'), y, llama_resp, progress_bars[y], system, instruct, indexes_analized, indexes_errors))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatthen the list\n",
    "llama_resp_no_core = []\n",
    "for resp in llama_resp:\n",
    "    llama_resp_no_core.append(resp[1])\n",
    "\n",
    "llama_resp_no_core = list(itertools.chain.from_iterable(llama_resp_no_core))\n",
    "llama_resp_no_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean responses and find errors\n",
    "llama_resp_clean = []\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i, resp in enumerate(llama_resp_no_core):\n",
    "    if \"LAST ERROR\" in resp:\n",
    "        llama_resp_clean.append(\"error\")\n",
    "        errors.append(i)\n",
    "    else:\n",
    "        # Extract last line, after the last \\n\n",
    "        lines = resp.split(\"\\n\")\n",
    "        last_line = lines[-1]\n",
    "        llama_resp_clean.append(last_line)\n",
    "\n",
    "    \n",
    "print(llama_resp_clean)\n",
    "print(errors)\n",
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct errors\n",
    "candidates_errors = split_list([(index, f\"Title: {row['Title']}\\n Abstract: {row['Abstract']}\") for index, row in df_selected.iterrows() if index in errors], workers)\n",
    "corrected_errors = [[] for _ in range(workers)]\n",
    "indexes_analized = [[] for _ in range(workers)]\n",
    "indexes_errors = [[] for _ in range(workers)]\n",
    "\n",
    "progress_bars = [tqdm(total=len(candidates_errors[j]), desc=f\"Progress {j}\", leave=True) for j in range(len(candidates_errors))]\n",
    "\n",
    "\n",
    "for y in range(workers):\n",
    "    thread = threading.Thread(target=correct_llama_parallel, args=(candidates_errors[y], eval(f'client{y+1}'), y, corrected_errors, progress_bars[y], system, instruct, indexes_analized, indexes_errors))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Flatten the list of errors\n",
    "corrected_errors_no_core = []\n",
    "for error in corrected_errors:\n",
    "    corrected_errors_no_core.append(error[1])\n",
    "\n",
    "corrected_errors_no_core = list(itertools.chain.from_iterable(corrected_errors_no_core))\n",
    "\n",
    "for i, error in enumerate(errors):\n",
    "    llama_resp_no_core[error] = corrected_errors_no_core[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for resp in llama_resp_clean:\n",
    "    if \"none\" in resp.lower():\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected['normalized'] = llama_resp_clean\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.to_csv('../1_data/selected_papers_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_semantics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
